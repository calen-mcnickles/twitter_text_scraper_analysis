{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Twitter, Clean Tweets, Analyze results\n",
    "Calen McNickles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Scrape most recent designated number of tweets and export them to sqlite db - recommended for larger scrapes. You can also use the scraped tweet df directly with the rest of the notebook, otherwise export the df directly to a csv - recommended for smaller scrapes.\n",
    "2. Clean all of the tweets text and get the df down to individual words and their count. \n",
    "3. Scrape specified number of coin names and tickers from coingecko.com.\n",
    "4. Count only occruences of coin names and tickers within the scraped tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Scrape by username and exports to sql db\n",
    "Credit to Martin Beck for the snscrape walklthrough - https://betterprogramming.pub/how-to-scrape-tweets-with-snscrape-90124ed006af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database used to export tweets\n",
    "engine = sqlalchemy.create_engine('sqlite:///twitter_crypto.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports list of twitter crypto influencer user names\n",
    "twitter_users = pd.read_csv('csv/twitter_crypto_influencers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Justin_Bons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CyberCapital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gavofyork</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>haydenzadams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MessariCrypto</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           names\n",
       "0    Justin_Bons\n",
       "1   CyberCapital\n",
       "2      gavofyork\n",
       "3   haydenzadams\n",
       "4  MessariCrypto"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loops through lists of tweeter users -> creates df with scraped tweets -> exports to SQL. Ideal for larger scrapes\n",
    "def twitter_scrape_usernames_to_db(max_tweets, name_df):\n",
    "    for name in user_list:\n",
    "        tweets_list1 = []\n",
    "        for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'from:{name}').get_items()):\n",
    "            if i > maxTweets:\n",
    "                break\n",
    "            tweets_list1.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "        tweets_df1 = pd.DataFrame(tweets_list1, columns=['datetime', 'tweet_id', 'text', 'username']) # creates df of tweets\n",
    "        tweets_df1.to_sql('tweets', engine, if_exists='append', index=False) # exports to sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loops through lists of tweeter users -> creates df with scraped tweets. Ideal for smaller tweet scrapes\n",
    "def twitter_scrape_usernames(max_tweets, name_df):\n",
    "    max_tweets = max_tweets  \n",
    "    user_list = name_df['names'].tolist()    \n",
    "    tweets_list1 = []\n",
    "    for name in user_list:\n",
    "        for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'from:{name}').get_items()):\n",
    "            if i > max_tweets:\n",
    "                break\n",
    "            tweets_list1.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "    tweets_df1 = pd.DataFrame(tweets_list1, columns=['datetime', 'tweet_id', 'text', 'username']) # creates df of tweets\n",
    "    return tweets_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape tweets for each user on the twitter_users df and exports to sqlite db\n",
    "# twitter_scrape_usernames_to_db(1, twitter_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape tweets for each user on the twitter_users df\n",
    "tweets_df = twitter_scrape_usernames(1000, twitter_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export dataframe as csv\n",
    "# tweets_df.to_csv('user-tweets.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clean and count tweet keywords\n",
    "I found 'Alex The Analysts' video helpful if anyone is struggling with this section. https://www.youtube.com/watch?v=MpIi4HtCiVk&t=114s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "# df = pd.read_csv('user-tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove @usersnames from tweets  \n",
    "tweets_df['text']=tweets_df['text'].str.replace('(\\@\\w+.*?)',\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits all the sentences up into individual words\n",
    "list_of_sentences = [word for word in tweets_df.text]\n",
    "\n",
    "lines = []\n",
    "for line in list_of_sentences:    \n",
    "    words = line.split()\n",
    "    for w in words: \n",
    "       lines.append(w.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using regular expression to remove all punctuation\n",
    "lines = [re.sub(r'[^A-Za-z0-9]+','',x) for x in lines]\n",
    "lines2 = []\n",
    "\n",
    "for w in lines:\n",
    "    if w != '':\n",
    "        lines2.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stopwords list\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add stopwords to list, this step isnt necessary as we will be counting key words later in the notebook. Still useful if you want to make changes. \n",
    "new_stopwords = ['i','want','The','like','people','one','dont','get','this','know','good']\n",
    "\n",
    "for i in new_stopwords:\n",
    "    stop_words.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out stopwords\n",
    "filtered_sentence = [w for w in lines2 if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(filtered_sentence)\n",
    "top_words = df[0].value_counts().reset_index()\n",
    "top_words.columns = ['word','count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports to csv\n",
    "# df.to_csv('top_word_6_2020_6_2021.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Scrape coin names and tickers from coingecko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapes coin page, 100 per page, name & ticker from coin gecko and create a df\n",
    "def scape_coin_names(pages):\n",
    "    all_coins = []\n",
    "    for i in range(1,pages):\n",
    "        # request site\n",
    "        cypto_price = requests.get(f'https://www.coingecko.com/en?page={i}')\n",
    "        soup = BeautifulSoup(cypto_price.content, 'html.parser')\n",
    "\n",
    "        # find all coin names\n",
    "        names = soup.find_all('a',{'class': 'tw-hidden lg:tw-flex font-bold tw-items-center tw-justify-between'})\n",
    "\n",
    "        # find all coin tickers\n",
    "        tickers = soup.find_all('span',{'class':'tw-hidden d-lg-inline font-normal text-3xs ml-2'})\n",
    "\n",
    "        coin_name = [i.text.strip().lower() for i in names]\n",
    "        coin_tic = [i.text.strip().lower() for i in tickers]\n",
    "        alll = coin_name + coin_tic\n",
    "        all_coins += alll\n",
    "    df = pd.DataFrame(all_coins,columns=['coin'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all_coins df\n",
    "all_coins = scape_coin_names(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_coins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports coin name & tickers to csv\n",
    "# all_coins.to_csv('top_coin_names.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Count occurrence of top coin names and tickers in tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "# coin_list = pd.read_csv('top_coin_names.csv') # top coin names\n",
    "# top_word = pd.read_csv('top_word.csv') # top words used in twitter posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column in order to use merge\n",
    "coin_word = all_coins.rename(columns={'coin':'word'})\n",
    "coin_word.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge tables on coin name or ticker in order to see count \n",
    "coins_in_tweet = coin_word.merge(top_words,on='word', how='inner')\n",
    "coins_in_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group words as there appears to be duplicates\n",
    "group = coins_in_tweet.groupby(['word']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coins_in_tweet = group.sort_values('count',ascending=False)\n",
    "coins_in_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart of top words mentioned. Something to note, many of these words are commonly used words that are also either coin names or tickers. \n",
    "coins_in_tweet = coins_in_tweet[:20]\n",
    "plt.figure(figsize=(15,9))\n",
    "sns.barplot(x='count', y='word',data=coins_in_tweet)\n",
    "plt.title('Top Coins Mentioned')\n",
    "plt.ylabel('Word from Tweet', fontsize=12)\n",
    "plt.xlabel('Count of Words', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv\n",
    "# coins_in_tweet.to_csv('coins_in_tweet.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
